{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.data_utils import unzip, load_data\n",
    "from src.preprocess import merge, preprocess_ratings, preprocess_movies, preprocess_users\n",
    "from benchmark.evaluate import test_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted to data/interim/\n"
     ]
    }
   ],
   "source": [
    "def get_train_test(base='u1'):\n",
    "    users, items, ratings_train = load_data(f'{base}.base')\n",
    "    users, items, ratings_test = load_data(f'{base}.test')\n",
    "\n",
    "    users, items = preprocess_users(users), preprocess_movies(items)\n",
    "    ratings_train, ratings_test = preprocess_ratings(ratings_train), preprocess_ratings(ratings_test)\n",
    "\n",
    "    data_train = merge(ratings_train, users, items)\n",
    "    data_test = merge(ratings_test, users, items)\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "unzip()\n",
    "\n",
    "bases = ['u1','u2','u3','u4','u5', 'ua', 'ub']\n",
    "data = []\n",
    "\n",
    "for base in bases:\n",
    "    data.append(get_train_test(base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print number of users and movies used in each part of data u1..u5 base and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base|Train Movies|Test Movie|Train users|Test users\n",
      "u1 1650 1410 943 459\n",
      "u2 1648 1420 943 653\n",
      "u3 1650 1423 943 869\n",
      "u4 1660 1394 943 923\n",
      "u5 1650 1407 943 927\n",
      "ua 1680 1129 943 943\n",
      "ub 1675 1145 943 943\n"
     ]
    }
   ],
   "source": [
    "print('Base|Train Movies|Test Movie|Train users|Test users')\n",
    "for base, (train, test) in zip(bases,data):\n",
    "    print(base, train['movie_id'].nunique(),test['movie_id'].nunique(), train['user_id'].nunique(),test['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training and testing\n",
    " Calculate RMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: u1, RMSE: 1.0205. MAE: 0.8156\n",
      "Base: u2, RMSE: 1.0060. MAE: 0.8011\n",
      "Base: u3, RMSE: 0.9963. MAE: 0.7930\n",
      "Base: u4, RMSE: 1.0041. MAE: 0.7999\n",
      "Base: u5, RMSE: 1.0066. MAE: 0.8023\n",
      "Base: ua, RMSE: 1.0311. MAE: 0.8252\n",
      "Base: ub, RMSE: 1.0429. MAE: 0.8316\n",
      "Avg MAE: 0.8097977482102579. Avg RMSE: 1.015340711553398\n"
     ]
    }
   ],
   "source": [
    "maes, rmses = [], []\n",
    "models = {}\n",
    "for base, (data_train, test_data) in zip(bases, data):\n",
    "    train, test = data_train.drop(columns=['user_id', 'movie_id']), test_data.drop(columns=['user_id', 'movie_id'])\n",
    "\n",
    "    X_train = train.drop('rating', axis=1)\n",
    "    X_test = test.drop('rating', axis=1)\n",
    "    y_train = train['rating']\n",
    "    y_test = test['rating']\n",
    "\n",
    "    # Rescale from 1 to 5\n",
    "    y_train = y_train*4+1\n",
    "    y_test = y_test*4+1\n",
    "\n",
    "    model = RandomForestRegressor(random_state=123)\n",
    "    model.fit(X_train, y_train)\n",
    "    models[base]=model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = mean_squared_error(y_true=y_test, y_pred=predictions)**0.5\n",
    "\n",
    "    print(f'Base: {base}, RMSE: {rmse:.4f}. MAE: {mae:.4f}')  \n",
    "\n",
    "    maes.append(mae)\n",
    "    rmses.append(rmse)\n",
    "\n",
    "print(f'Avg MAE: {np.mean(maes)}. Avg RMSE: {np.mean(rmses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many ratings each user gave in the different test sets\n",
    "Measure mean, median and min to understand what is more less suitable K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: u1, Mean: 43.572985, Med: 25.0, Min: 1\n",
      "Base: u2, Mean: 30.627871, Med: 18.0, Min: 1\n",
      "Base: u3, Mean: 23.014960, Med: 13.0, Min: 1\n",
      "Base: u4, Mean: 21.668472, Med: 12.0, Min: 1\n",
      "Base: u5, Mean: 21.574973, Med: 11.0, Min: 1\n",
      "Base: ua, Mean: 10.000000, Med: 10.0, Min: 10\n",
      "Base: ub, Mean: 10.000000, Med: 10.0, Min: 10\n"
     ]
    }
   ],
   "source": [
    "for base in bases:\n",
    "    um = load_data(f'{base}.test')[2][['user_id', \"movie_id\"]]\n",
    "    l = list(um.groupby(by=['user_id']))\n",
    "\n",
    "    sizes = []\n",
    "    for i in l:\n",
    "        sizes.append(i[1].shape[0])\n",
    "\n",
    "    mi, me, min = np.mean(sizes), np.median(sizes), np.min(sizes)\n",
    "    print(f'Base: {base}, Mean: {mi:f}, Med: {me}, Min: {min}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate recall for different test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: u1. Avg recall: 0.535577\n",
      "Base: u2. Avg recall: 0.636331\n",
      "Base: u3. Avg recall: 0.715343\n",
      "Base: u4. Avg recall: 0.742827\n",
      "Base: u5. Avg recall: 0.757044\n",
      "Mean recall on different dataset parts: 0.6774\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "liked_rating = 4\n",
    "\n",
    "total_recall = []\n",
    "for base in ['u1','u2','u3','u4','u5']:\n",
    "    recalls = test_recall(base, k=k, liked_rating=liked_rating, model=models[base])\n",
    "    mean = np.mean(recalls)\n",
    "    print(f'Base: {base}. Avg recall: {mean:4f}')\n",
    "    total_recall.append(mean)\n",
    "\n",
    "print(f'Mean recall on different dataset parts: {np.mean(total_recall):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize recal, mae and rmse for different bases (u1..u5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
