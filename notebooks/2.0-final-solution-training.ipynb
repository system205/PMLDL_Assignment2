{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.data_utils import unzip, load_data\n",
    "from src.preprocess import merge, preprocess_ratings, preprocess_movies, preprocess_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(base='u1'):\n",
    "    users, items, ratings_train = load_data(f'{base}.base')\n",
    "    users, items, ratings_test = load_data(f'{base}.test')\n",
    "\n",
    "    users, items = preprocess_users(users), preprocess_movies(items)\n",
    "    ratings_train, ratings_test = preprocess_ratings(ratings_train), preprocess_ratings(ratings_test)\n",
    "\n",
    "    data_train = merge(ratings_train, users, items)\n",
    "    data_test = merge(ratings_test, users, items)\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = ['u1','u2','u3','u4','u5', 'ua', 'ub']\n",
    "data = []\n",
    "\n",
    "for base in bases:\n",
    "    data.append(get_train_test(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base|Train Movies|Test Movie|Train users|Test users\n",
      "u1 1650 1410 943 459\n",
      "u2 1648 1420 943 653\n",
      "u3 1650 1423 943 869\n",
      "u4 1660 1394 943 923\n",
      "u5 1650 1407 943 927\n",
      "ua 1680 1129 943 943\n",
      "ub 1675 1145 943 943\n"
     ]
    }
   ],
   "source": [
    "print('Base|Train Movies|Test Movie|Train users|Test users')\n",
    "for base, (train, test) in zip(bases,data):\n",
    "    print(base, train['movie_id'].nunique(),test['movie_id'].nunique(), train['user_id'].nunique(),test['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training and testing\n",
    " Calculate RMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: u1, RMSE: 1.0205. MAE: 0.8156\n",
      "Base: u2, RMSE: 1.0060. MAE: 0.8011\n",
      "Base: u3, RMSE: 0.9963. MAE: 0.7930\n",
      "Base: u4, RMSE: 1.0041. MAE: 0.7999\n",
      "Base: u5, RMSE: 1.0066. MAE: 0.8023\n",
      "Base: ua, RMSE: 1.0311. MAE: 0.8252\n",
      "Base: ub, RMSE: 1.0429. MAE: 0.8316\n",
      "Avg MAE: 0.8097977482102579. Avg RMSE: 1.015340711553398\n"
     ]
    }
   ],
   "source": [
    "maes, rmses = [], []\n",
    "models = {}\n",
    "for base, (data_train, test_data) in zip(bases, data):\n",
    "    train, test = data_train.drop(columns=['user_id', 'movie_id']), test_data.drop(columns=['user_id', 'movie_id'])\n",
    "\n",
    "    X_train = train.drop('rating', axis=1)\n",
    "    X_test = test.drop('rating', axis=1)\n",
    "    y_train = train['rating']\n",
    "    y_test = test['rating']\n",
    "\n",
    "    # Rescale from 1 to 5\n",
    "    y_train = y_train*4+1\n",
    "    y_test = y_test*4+1\n",
    "\n",
    "    model = RandomForestRegressor(random_state=123)\n",
    "    model.fit(X_train, y_train)\n",
    "    models[base]=model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = mean_squared_error(y_true=y_test, y_pred=predictions)**0.5\n",
    "\n",
    "    print(f'Base: {base}, RMSE: {rmse:.4f}. MAE: {mae:.4f}')  \n",
    "\n",
    "    maes.append(mae)\n",
    "    rmses.append(rmse)\n",
    "\n",
    "print(f'Avg MAE: {np.mean(maes)}. Avg RMSE: {np.mean(rmses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for produce test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_with_all_movies(user_id, max_movie=1682, max_time=893286638, exclude_movies=set(), include_movies=None):\n",
    "    '''Return a dataframe of a user combined with possible movies for prediction of rating'''\n",
    "    data = []\n",
    "    for m in range(1, max_movie+1):\n",
    "        if ((include_movies is not None) and (m not in include_movies)): continue\n",
    "        elif (m in exclude_movies): continue\n",
    "\n",
    "        data.append([user_id, m, max_time])\n",
    "\n",
    "    all_ratings = pd.DataFrame(data, columns=['user_id', 'movie_id', 'timestamp'])\n",
    "\n",
    "    return all_ratings\n",
    "\n",
    "def get_for_test(base='u1'):\n",
    "    '''Returns ready to predict on list of dataframes with the specif user in each combined with possible movies'''\n",
    "    users, items, ratings_train = load_data(f'{base}.base')\n",
    "    users, items, ratings = load_data(f'{base}.test')\n",
    "\n",
    "    users, items = preprocess_users(users), preprocess_movies(items)\n",
    "\n",
    "    test_users = set(ratings['user_id'])\n",
    "\n",
    "    data_for_test = []\n",
    "    for u in test_users:\n",
    "        # Exclude movies of the user that were in train and include only test\n",
    "        exclude_movies = set(ratings_train[ratings_train['user_id']==u]['movie_id'])\n",
    "        include_movies = set(ratings[ratings['user_id']==u]['movie_id'])\n",
    "\n",
    "        ratings_test = combine_with_all_movies(u, exclude_movies=exclude_movies, include_movies=include_movies)\n",
    "        ratings_test = preprocess_ratings(ratings_test, has_rating=False)\n",
    "        \n",
    "        test_data = merge(ratings_test, users, items)\n",
    "\n",
    "        data_for_test.append(test_data)\n",
    "\n",
    "    return data_for_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test recall function\n",
    "Algo:\n",
    "- Pick a user\n",
    "- Iterate over test films and predict a rating\n",
    "- Pick top K predicted movies based on the rating\n",
    "- Calculate user recall: Check the present of each predicted film in the list of positively rated movies of this user\n",
    "- Iterate over all users and average the recall for all user in this test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_recall(base, k=20, liked_rating=4, model=None):\n",
    "    '''Return average recall for user in the base test set'''\n",
    "    recalls = []\n",
    "    for test_user in get_for_test(base=base):\n",
    "        X_test = test_user\n",
    "\n",
    "        movie_ids = X_test['movie_id']\n",
    "        user_id = X_test['user_id'][0]\n",
    "\n",
    "        X_test = X_test.drop(columns=['user_id', 'movie_id'])\n",
    "\n",
    "        # Predict rating for each film\n",
    "        film_rating = [(m, r) for m, r in zip(movie_ids, model.predict(X_test))]\n",
    "        film_rating = sorted(film_rating, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "        # Get top k rated films\n",
    "        top_k = [m for m,_ in film_rating[:k]]\n",
    "\n",
    "        # Get liked movies by the user\n",
    "        _, _, ratings = load_data(f'{base}.test')\n",
    "        liked_movies = list(ratings[(ratings['user_id']==user_id) & \\\n",
    "                                    (ratings['rating']>=liked_rating)]['movie_id'])   \n",
    "\n",
    "        # Check how many predicted films are in the liked list\n",
    "        hit = 0\n",
    "        for m in top_k:\n",
    "            if m in liked_movies: hit +=1\n",
    "        \n",
    "        # print(liked_movies)\n",
    "        # print(film_rating[:k])\n",
    "        recall = hit/len(liked_movies) if len(liked_movies) != 0 else 1\n",
    "        # print(f\"Recall for user {user_id}: {recall:.4f}\")\n",
    "        recalls.append(recall)\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how many ratings each user gave in the different test sets\n",
    "Measure mean, median and min to understand what is more less suitable K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: u1, Mean: 43.572985, Med: 25.0, Min: 1\n",
      "Base: u2, Mean: 30.627871, Med: 18.0, Min: 1\n",
      "Base: u3, Mean: 23.014960, Med: 13.0, Min: 1\n",
      "Base: u4, Mean: 21.668472, Med: 12.0, Min: 1\n",
      "Base: u5, Mean: 21.574973, Med: 11.0, Min: 1\n",
      "Base: ua, Mean: 10.000000, Med: 10.0, Min: 10\n",
      "Base: ub, Mean: 10.000000, Med: 10.0, Min: 10\n"
     ]
    }
   ],
   "source": [
    "for base in bases:\n",
    "    um = load_data(f'{base}.test')[2][['user_id', \"movie_id\"]]\n",
    "    l = list(um.groupby(by=['user_id']))\n",
    "\n",
    "    sizes = []\n",
    "    for i in l:\n",
    "        sizes.append(i[1].shape[0])\n",
    "\n",
    "    mi, me, min = np.mean(sizes), np.median(sizes), np.min(sizes)\n",
    "    print(f'Base: {base}, Mean: {mi:f}, Med: {me}, Min: {min}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate recall for different test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: u1. Avg recall: 0.535577\n",
      "Base: u2. Avg recall: 0.636331\n",
      "Base: u3. Avg recall: 0.715343\n",
      "Base: u4. Avg recall: 0.742827\n",
      "Base: u5. Avg recall: 0.757044\n",
      "Mean recall on different dataset parts: 0.6774\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "liked_rating = 4\n",
    "\n",
    "total_recall = []\n",
    "for base in ['u1','u2','u3','u4','u5']:\n",
    "    recalls = test_recall(base, k=k, liked_rating=liked_rating, model=models[base])\n",
    "    mean = np.mean(recalls)\n",
    "    print(f'Base: {base}. Avg recall: {mean:4f}')\n",
    "    total_recall.append(mean)\n",
    "\n",
    "print(f'Mean recall on different dataset parts: {np.mean(total_recall):.4f}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
